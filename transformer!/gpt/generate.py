import torch
import torch.nn.functional as F
import pickle
from model import GPT, GPTConfig

# åŠ è½½è¯è¡¨
with open("transformer!/gpt/vocab.pkl", "rb") as f:
    stoi, itos = pickle.load(f)

def encode(s):
    return [stoi.get(w, stoi.get('<unk>', 0)) for w in s.split()]

def decode(l):
    words = [itos[i] for i in l]
    return ' '.join(w for w in words if w not in {'<unk>', '<eos>', '<s>'})


def load_model(model_path, device, block_size=64):
    vocab_size = len(stoi)
    config = GPTConfig(
        vocab_size=vocab_size,
        block_size=block_size,
        n_layer=8,
        n_head=8,
        n_embd=256,
        dropout=0.1
    )
    model = GPT(config).to(device)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model, config

@torch.no_grad()
def generate(model, config, prompt, device, max_new_tokens=80, temperature=0.7, top_p=0.85, do_sample=True):
    # å°†è¾“å…¥çš„promptç¼–ç ä¸ºtensorï¼Œå¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ä¸Š
    input_ids = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)
    # å…‹éš†input_idsä»¥é¿å…ä¿®æ”¹åŸå§‹è¾“å…¥
    idx = input_ids.clone()
    # è·å–è¾“å…¥çš„é•¿åº¦
    input_len = input_ids.shape[1]

    # å¾ªç¯ç”Ÿæˆæ–°çš„tokenï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§é•¿åº¦æˆ–é‡åˆ°ç»“æŸç¬¦
    for _ in range(max_new_tokens):
        # è·å–å½“å‰è¾“å…¥çš„æœ€åblock_sizeä¸ªtoken
        idx_cond = idx[:, -config.block_size:]
        # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆlogits
        logits = model(idx_cond)
        # å¯¹logitsè¿›è¡Œç¼©æ”¾
        logits = logits[:, -1, :] / temperature
        # è®¡ç®—softmaxæ¦‚ç‡
        probs = F.softmax(logits, dim=-1)

        # å¦‚æœé€‰æ‹©é‡‡æ ·ç”Ÿæˆ
        if do_sample:
            # å¯¹æ¦‚ç‡è¿›è¡Œé™åºæ’åº
            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
            # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
            # è·å–ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
            cutoff = cumulative_probs > top_p
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªtokençš„æ¦‚ç‡ä¸ä¸º0
            cutoff[..., 1:] = cutoff[..., :-1].clone()
            cutoff[..., 0] = False
            # å°†è¶…è¿‡top_pçš„tokenæ¦‚ç‡è®¾ä¸º0
            sorted_probs[cutoff] = 0
            # å½’ä¸€åŒ–æ¦‚ç‡
            sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)
            # ä½¿ç”¨å¤šé¡¹å¼åˆ†å¸ƒé‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            next_token = torch.multinomial(sorted_probs, num_samples=1)
            # è·å–å¯¹åº”çš„tokenç´¢å¼•
            next_token = sorted_indices.gather(-1, next_token)
        else:
            # å¦‚æœä¸é‡‡æ ·ï¼Œé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„token
            next_token = torch.argmax(probs, dim=-1, keepdim=True)

        # å°†ç”Ÿæˆçš„tokenæ·»åŠ åˆ°å½“å‰åºåˆ—ä¸­
        idx = torch.cat((idx, next_token), dim=1)

        # å¦‚æœç”Ÿæˆ <eos>ï¼Œæå‰ç»“æŸ
        if next_token.item() == stoi.get("<eos>", -1):
            break

    # åªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬ promptï¼‰
    generated = idx[0, input_len:]
    return decode(generated.tolist())


if __name__ == "__main__":
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model, config = load_model("transformer!/gpt/gpt_model.pt", device)

    while True:
        user_input = input("ä½ ï¼š").strip()
        if user_input.lower() == "exit":
            break

        prompt = f"User: {user_input} Bot:"
        response = generate(model, config, prompt, device)
        print("ğŸ¤–ï¼š" + response)

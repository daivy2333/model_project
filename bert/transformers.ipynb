{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入transformers库\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 加载预训练的BERT模型和tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset= load_dataset('glue', 'sst2')\n",
    "# 定义函数预处理数据集\n",
    "\n",
    "def preprocess(example):\n",
    "    # 分词\n",
    "    # 使用tokenizer对输入的句子进行分词，同时进行填充和截断处理\n",
    "    # 'max_length'表示填充到最大长度，'truncation=True'表示启用截断，'max_length=128'表示最大长度为128\n",
    "    # 'return_tensors='pt''表示返回PyTorch张量\n",
    "    tokens= tokenizer(example['sentence'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "    # 返回特征\n",
    "    # 返回一个字典，包含三个键值对：\n",
    "    # 'input_ids'：输入的token ID，使用squeeze()方法去除维度为1的维度\n",
    "    # 'attention_mask'：注意力掩码，使用squeeze()方法去除维度为1的维度\n",
    "    # 'label'：示例的标签\n",
    "    return {'input_ids': tokens['input_ids'].squeeze(), 'attention_mask': tokens['attention_mask'].squeeze(), 'label': example['label']}\n",
    "\n",
    "\n",
    "dataset= dataset.map(preprocess)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size= 32\n",
    "seed= 42\n",
    "\n",
    "# 为数据集的每一个分割创建一个数据加载器\n",
    "\n",
    "dataloaders= {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    # 将特征转换为张量\n",
    "    input_ids= torch.tensor(dataset[split]['input_ids'])\n",
    "    attention_mask= torch.tensor(dataset[split]['attention_mask'])\n",
    "    labels= torch.tensor(dataset[split]['label'])\n",
    "\n",
    "    # 创建一个TensorDataset对象，将特征和标签组合在一起\n",
    "    tensor_dataset= TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "    # 创建一个数据加载器，将数据集分成多个批次\n",
    "    dataloader= DataLoader(tensor_dataset, batch_size=batch_size, shuffle=split=='train', num_workers= 4)\n",
    "\n",
    "    dataloaders[split]= dataloader\n",
    "\n",
    "    \n",
    "# 检查数据加载器是否正确创建\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print(f\"{split} dataloader created with {len(dataloaders[split])} batches.\")\n",
    "# 从每个数据加载器中取一个批次\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    print(f\"Checking {split} dataloader...\")\n",
    "    for batch in dataloaders[split]:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "        print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        break  # 只检查一个批次\n",
    "# 定义分类模型\n",
    "from torch import nn\n",
    "\n",
    "class BertSentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert= BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # 添加一个线性层\n",
    "        self.classifier= nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 使用BERT模型获取最后一层的隐藏状态\n",
    "        outputs= self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # 获取最后一层的隐藏状态\n",
    "        cls_embeddings= outputs.last_hidden_state[:, 0, :]\n",
    "        # 使用线性层进行分类\n",
    "        logits= self.classifier(cls_embeddings)\n",
    "\n",
    "        return logits\n",
    "# 设置训练参数与设备\n",
    "\n",
    "model= BertSentimentClassifier()\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer= torch.optim.AdamW(model.parameters(), lr= 2e-5)\n",
    "loss_fn= nn.CrossEntropyLoss()\n",
    "# 编写循环\n",
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    # 将模型设置为训练模式\n",
    "    model.train()\n",
    "    # 初始化总损失为0\n",
    "    total_loss= 0\n",
    "\n",
    "    # 遍历数据加载器中的每个批次\n",
    "    for batch in dataloader:\n",
    "        # 将输入数据、注意力掩码和标签移动到指定的设备（如GPU）\n",
    "        input_ids= batch[0].to(device)\n",
    "        attention_mask= batch[1].to(device)\n",
    "        labels= batch[2].to(device)\n",
    "\n",
    "        # 在每次迭代前清零优化器的梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 通过模型前向传播得到预测的logits\n",
    "        logits= model(input_ids, attention_mask)\n",
    "\n",
    "        # 计算损失\n",
    "        loss= loss_fn(logits, labels)\n",
    "\n",
    "        # 反向传播计算梯度\n",
    "        loss.backward()\n",
    "\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累加当前批次的损失\n",
    "        total_loss+= loss.item()\n",
    "\n",
    "    # 返回平均损失\n",
    "    return total_loss / len(dataloader)\n",
    "# 验证测试函数\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    # 将模型设置为评估模式，这样模型中的dropout和batchnorm层会工作在不同的方式\n",
    "    model.eval()\n",
    "\n",
    "    \n",
    "    # 初始化总损失和正确预测的计数器\n",
    "    total_loss, correct= 0, 0\n",
    "\n",
    "    # 使用torch.no_grad()上下文管理器，这样在评估过程中不会计算梯度，节省内存和计算资源\n",
    "    with torch.no_grad():\n",
    "        # 遍历dataloader中的每个批次数据\n",
    "        for batch in dataloader:\n",
    "            # 将输入数据、注意力掩码和标签移动到指定的设备（如GPU）\n",
    "            input_ids= batch[0].to(device)\n",
    "            attention_mask= batch[1].to(device)\n",
    "            labels= batch[2].to(device)\n",
    "\n",
    "            # 通过模型前向传播得到预测的logits\n",
    "            logits= model(input_ids, attention_mask)\n",
    "\n",
    "            # 计算当前批次的损失\n",
    "            loss= loss_fn(logits, labels)\n",
    "            # 累加当前批次的损失到总损失中\n",
    "            total_loss+= loss.item()\n",
    "\n",
    "            # 获取预测结果，即logits中最大值的索引\n",
    "            preds= torch.argmax(logits, dim=1)\n",
    "            # 计算当前批次中预测正确的样本数，并累加到正确预测的计数器中\n",
    "            correct+= (preds==labels).sum().item()\n",
    "    \n",
    "    acccuracy= correct / len(dataloader.dataset)\n",
    "    return total_loss / len(dataloader), acccuracy\n",
    "# 训练模型\n",
    "\n",
    "epochs= 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss= train_epoch(model, dataloaders['train'], loss_fn, optimizer, device)\n",
    "\n",
    "    val_loss, val_accuracy= evaluate(model, dataloaders['validation'], loss_fn, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertSentimentClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert_sst2_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\pytorch\\Lib\\site-packages\\torch\\serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 628\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\miniconda\\envs\\pytorch\\Lib\\site-packages\\torch\\serialization.py:859\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 859\u001b[0m     storage \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    860\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[1;32md:\\miniconda\\envs\\pytorch\\Lib\\site-packages\\torch\\storage.py:137\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'bert_sst2_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库（补充部分）\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, AdamW\n",
    "\n",
    "# 模型定义、训练函数、评估函数（如上述代码）\n",
    "\n",
    "# 主流程\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化模型、优化器、设备\n",
    "    model = BertSentimentClassifier().to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(3):\n",
    "        train_loss = train_epoch(...)\n",
    "        val_loss, val_acc = evaluate(...)\n",
    "        print(f\"Epoch {epoch+1} Results...\")\n",
    "    \n",
    "    # 测试评估\n",
    "    test_loss, test_acc = evaluate(...)\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

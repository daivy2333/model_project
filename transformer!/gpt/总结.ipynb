{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bcad126",
   "metadata": {},
   "source": [
    "这次能跑起来就算成功，但是最后得到的远远超出我的预期，所以在此逐步总结，好彻底结尾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66870c87",
   "metadata": {},
   "source": [
    "首先在这里感谢gpt爹的全力帮助，感谢你不厌其烦地给我解答，谢谢你\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bd7b0",
   "metadata": {},
   "source": [
    "## 数据集准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e088f5",
   "metadata": {},
   "source": [
    "我是在kaggle上面搜索的数据集，是ted的演讲csv，搜索数据集应该不算难事，我仅仅在这里附上网站\n",
    "https://www.kaggle.com/datasets/rounakbanik/ted-talks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece873a",
   "metadata": {},
   "source": [
    "### 数据清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca24eb6",
   "metadata": {},
   "source": [
    "我想下面一个函数能解释一切"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f51f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # 去除所有 (...) 和 [...] 中的内容，包括括号本身\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    # 替换多个空格为一个空格\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6402044",
   "metadata": {},
   "source": [
    "我把csv放到一个文件夹里面了方便集中处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0788c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 遍历文件夹下所有 CSV 文件\n",
    "for csv_file in csv_dir.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # 自动选择合适的文本列\n",
    "    column = 'transcript' if 'transcript' in df.columns else 'content' if 'content' in df.columns else None\n",
    "    if column is None:\n",
    "        print(f\"⚠️ 跳过文件（无合适文本列）: {csv_file.name}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f44bb67",
   "metadata": {},
   "source": [
    "具体实现在代码里面有，再次感谢gpt爹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57eb3b",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840eb86",
   "metadata": {},
   "source": [
    "这里已经得到了初步可用的数据，但是为了增强数据可用性，需要进行预处理增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12401199",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. 添加特殊 token：<s> 开头，<eos> 结尾\n",
    "# 将每一行处理为句子（可根据需要替换为其他分割方法）\n",
    "lines = raw_text.strip().splitlines()\n",
    "tokenized_lines = [[\"<s>\"] + line.strip().split() + [\"<eos>\"] for line in lines]\n",
    "words = [word for line in tokenized_lines for word in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b9931",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. 保存词表\n",
    "with open(\"transformer!/gpt/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump((stoi, itos), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd9863",
   "metadata": {},
   "source": [
    "用的词嵌入的方法，得到了词表vocab.pkl，这个文件在transformer/gpt文件夹下,但是还不够，需要将其使其编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da97d7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. 编码文本为 token ID\n",
    "def encode(word_list):\n",
    "    return [stoi.get(w, stoi[\"<unk>\"]) for w in word_list]\n",
    "\n",
    "data_ids = torch.tensor(encode(words), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6805185",
   "metadata": {},
   "source": [
    "然后才是划分训练集和验证集得到train和val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef703420",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 6. 划分训练/验证集\n",
    "n = int(0.9 * len(data_ids))\n",
    "train_data = data_ids[:n]\n",
    "val_data = data_ids[n:]\n",
    "\n",
    "# 7. 保存编码后的数据\n",
    "Path(\"transformer!/gpt/data\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(train_data, \"transformer!/gpt/data/train.pt\")\n",
    "torch.save(val_data, \"transformer!/gpt/data/val.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f90335",
   "metadata": {},
   "source": [
    "## 模型的构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94dd5f",
   "metadata": {},
   "source": [
    "定义好参数的东西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db6c39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    # 初始化方法，用于创建GPTConfig类的实例\n",
    "    def __init__(self, vocab_size, block_size=128, n_layer=4, n_head=4, n_embd=256, dropout=0.1):\n",
    "        # vocab_size: 词汇表大小，表示模型可以处理的唯一标记的数量\n",
    "        self.vocab_size = vocab_size\n",
    "        # block_size: 上下文窗口大小，表示模型在一次前向传播中可以处理的序列长度\n",
    "        self.block_size = block_size\n",
    "        # n_layer: Transformer层的数量，表示模型的深度\n",
    "        self.n_layer = n_layer\n",
    "        # n_head: 多头注意力机制中的注意力头数，表示每个注意力头负责的部分注意力计算\n",
    "        self.n_head = n_head\n",
    "        # n_embd: 嵌入维度，表示每个标记的向量表示的维度\n",
    "        self.n_embd = n_embd\n",
    "        # dropout: Dropout率，用于防止过拟合，表示在训练过程中随机丢弃的比例\n",
    "        self.dropout = dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d526b",
   "metadata": {},
   "source": [
    "关键的自注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ce2b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        # 初始化父类\n",
    "        super().__init__()\n",
    "        # 定义线性变换层，用于计算key\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # 定义线性变换层，用于计算query\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # 定义线性变换层，用于计算value\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # 定义线性变换层，用于输出投影\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # 定义dropout层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        # 定义注意力头的数量\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 获取输入的批次大小、序列长度和嵌入维度\n",
    "        B, T, C = x.size()\n",
    "        # 计算每个注意力头的维度\n",
    "        head_dim = C // self.n_head\n",
    "        # 计算key，并调整形状以适应多头注意力\n",
    "        k = self.key(x).view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
    "        # 计算query，并调整形状以适应多头注意力\n",
    "        q = self.query(x).view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
    "        # 计算value，并调整形状以适应多头注意力\n",
    "        v = self.value(x).view(B, T, self.n_head, head_dim).transpose(1, 2)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        att = (q @ k.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "        # 创建掩码矩阵，用于防止未来信息的泄露\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        # 将注意力分数中不需要的部分设为负无穷\n",
    "        att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        # 应用softmax函数，得到注意力权重\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        # 应用dropout\n",
    "        att = self.dropout(att)\n",
    "        # 计算输出\n",
    "        out = att @ v\n",
    "        # 调整输出形状\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # 应用输出投影\n",
    "        return self.proj(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a5d05",
   "metadata": {},
   "source": [
    "然后一个完整的transformer块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83bfe7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    # 定义TransformerBlock类，继承自nn.Module\n",
    "    def __init__(self, config):\n",
    "        # 初始化函数，接收配置参数config\n",
    "        super().__init__()\n",
    "        # 调用父类nn.Module的初始化函数\n",
    "        self.sa = SelfAttention(config)\n",
    "        # 初始化自注意力机制模块SelfAttention，传入配置参数config\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        # 初始化第一个层归一化模块LayerNorm，输入维度为config.n_embd\n",
    "        self.ff = nn.Sequential(\n",
    "            # 初始化前馈神经网络模块，包含以下层：\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            # 线性层，将输入维度从config.n_embd变换到4倍的config.n_embd\n",
    "            nn.ReLU(),\n",
    "            # ReLU激活函数\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            # 线性层，将输入维度从4倍的config.n_embd变换回config.n_embd\n",
    "            nn.Dropout(config.dropout),\n",
    "            # Dropout层，防止过拟合，丢弃率为config.dropout\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "        # 初始化第二个层归一化模块LayerNorm，输入维度为config.n_embd\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播函数，接收输入x\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # 计算自注意力机制输出，并进行残差连接，输入x经过第一个层归一化后传入自注意力模块\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        # 计算前馈神经网络输出，并进行残差连接，输入x经过第二个层归一化后传入前馈神经网络模块\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d845e",
   "metadata": {},
   "source": [
    "完整的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebde051",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        # 初始化父类nn.Module\n",
    "        super().__init__()\n",
    "        # 创建词嵌入层，将词汇表中的每个词映射到n_embd维的向量\n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        # 创建位置嵌入层，将序列中的每个位置映射到n_embd维的向量\n",
    "        self.pos_embed = nn.Embedding(config.block_size, config.n_embd)\n",
    "        # 创建多个TransformerBlock，并使用nn.Sequential将它们串联起来\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config.n_layer)])\n",
    "        # 创建层归一化层，用于对最后的输出进行归一化处理\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        # 创建线性层，将n_embd维的向量映射到词汇表大小的向量，用于生成预测\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # 获取输入的批次大小B和序列长度T\n",
    "        B, T = idx.size()\n",
    "        # 创建一个从0到T-1的序列，表示位置信息，并将其移动到与输入相同的设备上\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        # 将输入的索引通过词嵌入层和位置嵌入层，得到嵌入向量，并进行相加\n",
    "        x = self.token_embed(idx) + self.pos_embed(pos)\n",
    "        # 将嵌入向量通过多个TransformerBlock进行前向传播\n",
    "        x = self.blocks(x)\n",
    "        # 对最后的输出进行层归一化处理\n",
    "        x = self.ln_f(x)\n",
    "        # 将归一化后的输出通过线性层，得到最终的预测结果\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c4533",
   "metadata": {},
   "source": [
    "构建模型很简单啊是不是，其实都是gpt爹做的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce5b1f",
   "metadata": {},
   "source": [
    "## 准备训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05895aea",
   "metadata": {},
   "source": [
    "一些简单的配置，这里我用了cuda，没这个真的不行，还用了训练可视化tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d5b01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. 环境配置\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "writer = SummaryWriter(log_dir=\"transformer!/gpt/runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3537e5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. 加载词表\n",
    "with open(\"transformer!/gpt/vocab.pkl\", \"rb\") as f:\n",
    "    stoi, itos = pickle.load(f)\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7ac42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. 超参数配置\n",
    "block_size = 64  # 句子长度（单词数）\n",
    "n_layer = 8\n",
    "n_head = 8\n",
    "n_embd = 256\n",
    "dropout = 0.1\n",
    "lr = 3e-4\n",
    "batch_size = 32\n",
    "max_steps = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202d184",
   "metadata": {},
   "source": [
    "超参数是最关键的部分，要根据要求酌情调整，我最后因为断电影响，只能训练五万个周期，gpu也支撑不了更高的参数，但是已经足够了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b3fda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. 加载预处理数据\n",
    "train_data = torch.load(\"transformer!/gpt/data/train.pt\")\n",
    "val_data = torch.load(\"transformer!/gpt/data/val.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143346be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. 构建模型\n",
    "config = GPTConfig(vocab_size, block_size, n_layer, n_head, n_embd, dropout)\n",
    "model = GPT(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d01d06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 6. 生成训练批次\n",
    "def get_batch(data):\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e989c16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 7. 训练循环\n",
    "for step in range(max_steps):\n",
    "    x, y = get_batch(train_data)\n",
    "\n",
    "    # AMP 混合精度训练\n",
    "    with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    writer.add_scalar(\"Loss/Train\", loss.item(), step)\n",
    "\n",
    "    # 验证与打印\n",
    "    if step % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad(), autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            x_val, y_val = get_batch(val_data)\n",
    "            val_logits = model(x_val)\n",
    "            val_loss = F.cross_entropy(val_logits.view(-1, vocab_size), y_val.view(-1))\n",
    "        writer.add_scalar(\"Loss/Val\", val_loss.item(), step)\n",
    "        print(f\"Step {step}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "\n",
    "# 8. 保存模型\n",
    "torch.save(model.state_dict(), \"transformer!/gpt/gpt_model.pt\")\n",
    "print(\"✅ 训练完成，模型已保存\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa958e3",
   "metadata": {},
   "source": [
    "## 然后是解读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb996cdf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 加载词表\n",
    "with open(\"transformer!/gpt/vocab.pkl\", \"rb\") as f:\n",
    "    stoi, itos = pickle.load(f)\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi.get(w, stoi.get('<unk>', 0)) for w in s.split()]\n",
    "\n",
    "def decode(l):\n",
    "    words = [itos[i] for i in l]\n",
    "    return ' '.join(w for w in words if w not in {'<unk>', '<eos>', '<s>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920a0f2",
   "metadata": {},
   "source": [
    "这里有个刚才没讲到的关键东西unk, eos, s，有两个是用来断句的，还有一个用来处理陌生词汇，增强了鲁棒性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e61582c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, device, block_size=64):\n",
    "    vocab_size = len(stoi)\n",
    "    config = GPTConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        block_size=block_size,\n",
    "        n_layer=8,\n",
    "        n_head=8,\n",
    "        n_embd=256,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    model = GPT(config).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model, config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287907ba",
   "metadata": {},
   "source": [
    "以及最最关键的一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d11edf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, config, prompt, device, max_new_tokens=80, temperature=0.7, top_p=0.85, do_sample=True):\n",
    "    # 将输入的prompt编码为tensor，并移动到指定设备上\n",
    "    input_ids = torch.tensor(encode(prompt), dtype=torch.long)[None, :].to(device)\n",
    "    # 克隆input_ids以避免修改原始输入\n",
    "    idx = input_ids.clone()\n",
    "    # 获取输入的长度\n",
    "    input_len = input_ids.shape[1]\n",
    "\n",
    "    # 循环生成新的token，直到达到最大长度或遇到结束符\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 获取当前输入的最后block_size个token\n",
    "        idx_cond = idx[:, -config.block_size:]\n",
    "        # 使用模型生成logits\n",
    "        logits = model(idx_cond)\n",
    "        # 对logits进行缩放\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # 计算softmax概率\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # 如果选择采样生成\n",
    "        if do_sample:\n",
    "            # 对概率进行降序排序\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            # 计算累积概率\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            # 获取累积概率超过top_p的位置\n",
    "            cutoff = cumulative_probs > top_p\n",
    "            # 确保至少有一个token的概率不为0\n",
    "            cutoff[..., 1:] = cutoff[..., :-1].clone()\n",
    "            cutoff[..., 0] = False\n",
    "            # 将超过top_p的token概率设为0\n",
    "            sorted_probs[cutoff] = 0\n",
    "            # 归一化概率\n",
    "            sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "            # 使用多项式分布采样下一个token\n",
    "            next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
    "            # 获取对应的token索引\n",
    "            next_token = sorted_indices.gather(-1, next_token)\n",
    "        else:\n",
    "            # 如果不采样，选择概率最大的token\n",
    "            next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "\n",
    "        # 将生成的token添加到当前序列中\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "        # 如果生成 <eos>，提前结束\n",
    "        if next_token.item() == stoi.get(\"<eos>\", -1):\n",
    "            break\n",
    "\n",
    "    # 只返回生成的部分（不包括 prompt）\n",
    "    generated = idx[0, input_len:]\n",
    "    return decode(generated.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e7f9f",
   "metadata": {},
   "source": [
    "注意里面的参数，我没有特地拿出来，但实际上也是很重要的\n",
    "max_new_tokens=80, temperature=0.7, top_p=0.85, do_sample=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a066bac",
   "metadata": {},
   "source": [
    "什么降温升温的，参数不懂可以问gpt，可以极大地影响生成的东西"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b90bf",
   "metadata": {},
   "source": [
    "最后的主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4e074",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model, config = load_model(\"transformer!/gpt/gpt_model.pt\", device)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"你：\").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        prompt = f\"User: {user_input} Bot:\"\n",
    "        response = generate(model, config, prompt, device)\n",
    "        print(\"🤖：\" + response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728b7d5",
   "metadata": {},
   "source": [
    "最后我除了gpt爹我还要感谢我自己，感谢牢卢，我从两年前一无所知，到一年前刚接触ai，再到现在终于能跑一个智障，真是太艰难了，所以我要感谢。\n",
    "还有我的家人们，因为我的任性，买了很贵的电脑，这确实是一笔巨款，对我们家，但是他们还是买了，现在我终于用它做出了点成就，将来一定也能做更多，来回报一直以来的支持，感谢，爱你们"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import onnxruntime as ort
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch
import numpy as np

# è®¾ç½®è®¾å¤‡
device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½ ONNX æ¨¡å‹
ort_session = ort.InferenceSession("D:\\data_store\\onnx_model\\model.onnx")

# åŠ è½½åŸå§‹æ¨¡å‹å’Œå¤„ç†å™¨
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model.eval()

# åŠ è½½å›¾åƒ
image = Image.open("image_des_onnx\\assets\\ç¾å¥³çš„èƒŒå½±.png").convert("RGB")
inputs = processor(images=image, return_tensors="pt")
pixel_values = inputs["pixel_values"].cpu().numpy()

# ä½¿ç”¨ ONNX æå–è§†è§‰ç‰¹å¾
onnx_outputs = ort_session.run(None, {"pixel_values": pixel_values})
encoder_hidden_states = torch.tensor(onnx_outputs[0]).to(device)

# ä½¿ç”¨ text_decoder ç”Ÿæˆæè¿°
generated_ids = model.text_decoder.generate(
    encoder_hidden_states=encoder_hidden_states,
    attention_mask=torch.ones(encoder_hidden_states.shape[:2], dtype=torch.long).to(device),
    max_length=50
)

# è§£ç 
caption = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(f"ğŸ–¼ï¸ å›¾åƒæè¿°ï¼š{caption}")

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    # 定义一个名为DoubleConv的类，继承自nn.Module，用于实现两层卷积操作\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        # 构造函数，初始化DoubleConv类的实例\n",
    "        super().__init__()\n",
    "        # 调用父类nn.Module的构造函数，确保正确初始化\n",
    "        self.double_conv = nn.Sequential(\n",
    "            # 定义一个名为double_conv的顺序容器，包含一系列的卷积、批归一化和激活操作\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # 第一层卷积操作，输入通道数为in_channels，输出通道数为out_channels，卷积核大小为3x3，padding为1\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # 批归一化操作，对第一层卷积的输出进行归一化处理\n",
    "            nn.ReLU(inplace=True),\n",
    "            # ReLU激活函数，对归一化后的输出进行非线性变换，inplace=True表示在原地进行操作，节省内存\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # 第二层卷积操作，输入和输出通道数均为out_channels，卷积核大小为3x3，padding为1\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            # 批归一化操作，对第二层卷积的输出进行归一化处理\n",
    "            nn.ReLU(inplace=True)\n",
    "            # ReLU激活函数，对归一化后的输出进行非线性变换，inplace=True表示在原地进行操作，节省内存\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播函数，输入为x\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        # 初始化Down类，继承自nn.Module\n",
    "        super().__init__()\n",
    "        # 调用父类nn.Module的初始化方法\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            # 定义一个Sequential容器，包含多个子模块，按顺序执行\n",
    "            nn.MaxPool2d(2),\n",
    "            # 最大池化操作，池化窗口大小为2x2，步幅默认为2\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "            # DoubleConv是一个自定义的双卷积层模块，用于特征提取\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播方法\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "\n",
    "        # 初始化Up类，继承自nn.Module\n",
    "        # in_channels: 输入通道数\n",
    "        # out_channels: 输出通道数\n",
    "        # bilinear: 是否使用双线性上采样，默认为True\n",
    "        if bilinear:\n",
    "            # 如果使用双线性上采样\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            # nn.Upsample: 上采样层，scale_factor=2表示放大两倍，mode='bilinear'表示使用双线性插值，align_corners=True表示对齐角点\n",
    "        else:\n",
    "            # 如果不使用双线性上采样，使用转置卷积\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            # nn.ConvTranspose2d: 转置卷积层，in_channels为输入通道数，in_channels // 2为输出通道数，kernel_size=2, stride=2表示卷积核大小为2x2，步长为2\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "        # DoubleConv: 自定义的双卷积层，in_channels为输入通道数，out_channels为输出通道数\n",
    "    def forward(self, x1, x2):\n",
    "        # forward方法，定义前向传播过程\n",
    "        # x1: 输入特征图1\n",
    "        # x2: 输入特征图2\n",
    "        x1 = self.up(x1)\n",
    "        # 对x1进行上采样\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        # 计算x2和上采样后的x1在高度和宽度上的差异\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        # 使用F.pad对x1进行填充，使其尺寸与x2匹配\n",
    "        # 填充顺序为：[左, 右, 上, 下]\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        # 将x2和填充后的x1在通道维度上进行拼接\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):  # 定义一个名为OutConv的类，继承自nn.Module，用于输出卷积层\n",
    "    def __init__(self, in_channels, out_channels):  # 构造函数，初始化输出卷积层\n",
    "        super(OutConv, self).__init__()  # 调用父类nn.Module的构造函数\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)  # 初始化一个二维卷积层，卷积核大小为1\n",
    "\n",
    "    def forward(self, x):  # 定义前向传播函数\n",
    "        return self.conv(x)  # 对输入x进行卷积操作，并返回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
